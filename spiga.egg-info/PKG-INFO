Metadata-Version: 2.4
Name: spiga
Version: 0.0.6
Summary: SPIGA: Shape Preserving Facial Landmarks with Graph Attention Networks
Home-page: https://github.com/andresprados/SPIGA
Author: Andres Prados Torreblanca
Author-email: andresprator@gmail.com
License: BSD-3-Clause
Project-URL: Homepage, https://bmvc2022.mpi-inf.mpg.de/155/
Project-URL: SPIGA Paper, https://bmvc2022.mpi-inf.mpg.de/0155.pdf
Project-URL: Bug Tracker, https://github.com/andresprados/SPIGA/issues
Keywords: Computer Vision,Face Alignment,Head Pose Estimation,Pytorch,CNN,GNN,BMVC2022,WFLW,300W,Merlrav,COFW
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Education
Classifier: Operating System :: OS Independent
Classifier: Environment :: GPU
Classifier: Environment :: Console
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Scientific/Engineering :: Image Processing
Classifier: Topic :: Software Development :: Libraries
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: matplotlib>=3.2.1
Requires-Dist: numpy>=1.18.2
Requires-Dist: opencv-python>=4.2.0.32
Requires-Dist: Pillow>=7.0.0
Requires-Dist: torch>=1.4.0
Requires-Dist: torchvision>=0.5.0
Requires-Dist: torchaudio
Requires-Dist: scipy
Requires-Dist: scikit-learn
Provides-Extra: demo
Requires-Dist: retinaface-py>=0.0.2; extra == "demo"
Requires-Dist: sort-tracker-py>=1.0.2; extra == "demo"
Dynamic: license-file

# Inattentiveness Detection System ğŸ¯

This project implements a real-time multi-modal attentiveness monitoring system. It combines facial detection, head pose estimation, gaze estimation, and emotion recognition to **detect inattentiveness** in live video (e.g., webcam).

<video width="600" controls>
  <source src="media/rule4.mp4" type="video/mp4">
</video>

---

## ğŸ’¡ Features

The system detects inattentiveness based on 4 rule-based conditions:

1. **No Face Detected** â€“ The person leaves the camera view or occludes their face.
<img src="media/rule1.gif" width="400" alt="No Face Detected">

2. **Head Pose** â€“ Head rotation exceeds whatâ€™s needed to view the screen.
<img src="media/rule2.gif" width="400" alt="Head Pose Exceeds View Angle">

3. **Emotion Distraction** â€“ If the dominant emotion is not *neutral* with high confidence (>95%).
<img src="rule3.gif" width="400" alt="Emotion Based Detection">

4. **Gaze Deviation** â€“ Gaze not within calibrated screen bounds or saccade detected.
<img src="gaze.gif" width="400" alt="Gaze Deviation">


Then, over a 3-minute window, it computes how much of the time was inattentive:

| Inattentive Ratio | Status Label            |
|-------------------|-------------------------|
| > 25%             | Heavily Inattentive     |
| 15% â€“ 25%         | Medium Inattentive      |
| 5% â€“ 15%          | Slightly Inattentive    |
| â‰¤ 5%              | Mostly Attentive        |

---

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/fei430/inattentiveness-detector.git
cd inattentiveness-detector
```

### 2. Set Up Python Environment (Optional)

```bash
python -m venv venv
source venv/bin/activate         # on Windows: venv\Scripts\activate
```

### 3. Install PyTorch (according to your CUDA version)

Check your CUDA version:
```bash
nvidia-smi
```

Then install PyTorch:  
[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)

Example:
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 4. Install Dependencies

```bash
pip install -e .
pip install -e .[demo]

pip install -r requirements.txt
```

---

## ğŸ¥ Run the Demo

```bash
python run_demo_test.py
```


### ğŸ“¥ Download Pretrained Models

Before running the demo, please make sure to **download the required model weights** and place them in the correct directories:

| File Name         | Expected Location    | Download Link |
|------------------|----------------------|----------------|
| `resnet34.pt`     | `./models/`           | [Download](https://drive.google.com/file/d/1yvpPey14EgAzeOFH-AvyaBGJ1039z1g3/view?usp=drive_link) |
| `model_final.pt`  | `./models/spiga/`     | [Download](https://drive.google.com/file/d/1EYIRftQiC5QayO6DnpNX_JJJxV3HaCiu/view?usp=sharing) |
| `best_model.pth`  | `./models/`           | [Download](https://drive.google.com/file/d/1rKt0sEyuuX49lWtw9AmJHapbemdYSLjb/view?usp=sharing) |

> ğŸ“Œ Folder structure should look like:
```
project_root/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.pth
â”‚   â”œâ”€â”€ model_final.pt
â”œâ”€â”€ resnet34.pt
```



### Optional Arguments:
| Argument        | Default | Description |
|----------------|---------|-------------|
| `--input`      | 0       | Camera index or path to video |
| `--window`     | 180     | Sliding window in seconds |
| `--thresh1`    | 0.05    | Slight inattentiveness threshold |
| `--thresh2`    | 0.15    | Medium inattentiveness threshold |
| `--thresh3`    | 0.25    | Heavy inattentiveness threshold |
| `--dataset`    | wflw    | Landmark dataset for SPIGA |
| `--tracker`    | RetinaSort | Face tracker type |

---

## ğŸ§  Output Description

- `"INATTENTIVE"`: displayed in red when user is inattentive in current frame.
- `3-min Status`: printed based on last 3-minute analysis.
- Console also shows time ranges for inattentive segments like:

```
Inattentive segments:
From 14:53:10 to 14:53:23
From 14:54:01 to 14:54:17
```

---

## ğŸ“ Directory Overview

```
run_demo_test.py         # Main script
gaze.py                  # Gaze estimation + calibration
models/best_model.pth    # Emotion recognition model (ResEmoteNet)
approach/                # ResEmoteNet implementation
spiga/                   # SPIGA face and landmark detection
assets/                  # Input video or test samples (optional)
```

---

## ğŸ“Œ Notes

- Make sure `models/best_model.pth` exists and is compatible.
- You need a working webcam or valid video file to test.

---

## ğŸ“§ Contact

For questions, please reach out to Feiya Xiang or open an issue.
